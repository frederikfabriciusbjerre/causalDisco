% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tetrad_search.R
\docType{class}
\name{TetradSearch}
\alias{TetradSearch}
\title{TetradSearch R6 class}
\description{
High-level wrapper around the Java-based \strong{Tetrad} causal-discovery
library.  The class lets you choose independence tests, scores, and search
algorithms from Tetrad, run them on an R data set, and retrieve the
resulting graph or statistics.
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{data}}{Java object that stores the (possibly converted) data set
used by Tetrad.}

\item{\code{rdata}}{Original \strong{R} \code{data.frame} supplied by the user.}

\item{\code{score}}{Java object holding the scoring function selected with
\code{set_score()}. Supply one of the method strings for
\code{set_score()}. Recognised values are:

\itemize{
\item \code{"sem_bic"} – SEM BIC score.
\item \code{"ebic"} – Extended BIC score.
\item \code{"bdeu"} – Bayes Dirichlet Equivalent score with uniform priors.
\item \code{"basis_function_bic"} – BIC score for basis-function models. This is a generalization of the Degenerate Gaussian score.
\item \code{"conditional_gaussian"} – Mixed discrete/continuous BIC score.
\item \code{"degenerate_gaussian"} – Degenerate Gaussian BIC score.
\item \code{"discrete_bic"} – BIC score for discrete data.
\item \code{"gic"} – Generalized Information Criterion (GIC) score.
\item \code{"mag_degenerate_gaussian_bic"} – MAG Degenerate Gaussian BIC Score.
\item \code{"mixed_variable_polynomial"} – Mixed variable polynomial BIC score.
\item \code{"poisson_prior"} – Poisson prior score.
\item \code{"zhang_shen_bound"} – Gaussian Extended BIC score.
}}

\item{\code{test}}{Java object holding the independence test selected with
\code{set_test()}. Supply one of the method strings for
\code{set_test()}. Recognised values are:

\itemize{
\item \code{"chi_square"} – chi-squared test
\item \code{"g_square"}   – likelihood-ratio \(G^2\) test
\item \code{"basis_function_lrt"} – basis-function likelihood-ratio
\item \code{"probabilistic"} – Uses BCInference by Cooper and Bui to calculate probabilistic conditional independence judgments.
\item \code{"fisher_z"} – Fisher \(Z\) (partial correlation) test
\item \code{"degenerate_gaussian"} – Degenerate Gaussian test as a likelihood ratio test
\item \code{"cci"} – Conditional independence of variable in a continuous data set using Daudin's method.
\item \code{"conditional_gaussian"} – Mixed discrete/continuous test
\item \code{"kci"} – Kernel Conditional Independence Test (KCI) by Kun Zhang
}}

\item{\code{alg}}{Java object representing the search algorithm.
Supply one of the method strings for \code{set_alg()}.
Recognised values are:

\itemize{
\item \code{"boss"} – BOSS algorithm.
\item \code{"boss_fci"} – BOSS-FCI algorithm.
\item \code{"boss_pod"} – BOSS-POD (BOSS-PAG of DAG) algorithm.
\item \code{"ccd"} – Cyclic Causal Discovery.
\item \code{"cfci"} – Adjusts FCI to use conservative orientation as in CPC.
\item \code{"cpc"} – Conservative PC algorithm.
\item \code{"cstar"} – CStaR algorithm (Causal Stability Ranking).
\item \code{"dagma"} – DAGMA algorithm.
\item \code{"direct_lingam"} – DirectLiNGAM algorithm.
\item \code{"fask"} – FASK algorithm.
\item \code{"fci"} – FCI algorithm.
\item \code{"fges"} – Fast Greedy Equivalence Search (FGES) algorithm.
\item \code{"fges_mb"} – Fast Greedy Equivalence Search with Markov Blanket (FGES-MB) algorithm.
\item \code{"fofc"} – Find One Factor Clusters (FOFC)/MIMBUILD algorithm.
\item \code{"gfci"} – GFCI algorithm. Combines FGES and FCI.
\item \code{"grasp"} – GRaSP (Greedy Relations of Sparsest Permutation) algorithm.
\item \code{"grasp_fci"} – GRaSP-FCI algorithm. Combines GRaSP and FCI.
\item \code{"ica_lingam"} – ICA LiNGAM algorithm.
\item \code{"ica_lingd"} – ICA-LiNG-D algorithm
\item \code{"fcit"} – FCI Targeted Testing (FCIT) algorithm
\item \code{"pc"} – Peter-Clark (PC) algorithm
\item \code{"pc_max"} – PCMax algorithm
\item \code{"restricted_boss"} – Restricted BOSS algorithm
\item \code{"rfci"} – Restricted FCI algorithm
\item \code{"sp"} – Sparsest Permutation algorithm
\item \code{"sp_fci"} – Sparsest Permutation using FCI
\item \code{"svar_fci"} – SvarFCI algorithm (for timeseries data)
\item \code{"svar_gfci"} – SvarGFCI algorithm. Similar to SvarFCI, but uses a BIC score to search for a skeleton.
}}

\item{\code{mc_test}}{Java independence-test object used by the Markov checker.}

\item{\code{java}}{Java object returned by the search (typically a graph).}

\item{\code{result}}{Convenience alias for \code{java}; may store additional
metadata depending on the search type.}

\item{\code{knowledge}}{Java \code{Knowledge} object carrying background
constraints (required/forbidden edges).}

\item{\code{params}}{Java \code{Parameters} object holding algorithm settings.}

\item{\code{bootstrap_graphs}}{Java \code{List} of graphs produced by bootstrap
resampling, if that feature was requested.}

\item{\code{mc_ind_results}}{Java \code{List} with Markov-checker test results.}

\item{\code{bhat}}{Java object containing the \eqn{\hat B} adjacency matrix
(returned by some Tetrad algorithms).}

\item{\code{unstable_bhats}}{Collection of unstable \eqn{\hat B} matrices.}

\item{\code{stable_bhats}}{Collection of stable \eqn{\hat B} matrices.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-TetradSearch-new}{\code{TetradSearch$new()}}
\item \href{#method-TetradSearch-set_test}{\code{TetradSearch$set_test()}}
\item \href{#method-TetradSearch-set_score}{\code{TetradSearch$set_score()}}
\item \href{#method-TetradSearch-set_alg}{\code{TetradSearch$set_alg()}}
\item \href{#method-TetradSearch-set_knowledge}{\code{TetradSearch$set_knowledge()}}
\item \href{#method-TetradSearch-set_params}{\code{TetradSearch$set_params()}}
\item \href{#method-TetradSearch-get_parameters_for_function}{\code{TetradSearch$get_parameters_for_function()}}
\item \href{#method-TetradSearch-run_search}{\code{TetradSearch$run_search()}}
\item \href{#method-TetradSearch-set_bootstrapping}{\code{TetradSearch$set_bootstrapping()}}
\item \href{#method-TetradSearch-set_data}{\code{TetradSearch$set_data()}}
\item \href{#method-TetradSearch-set_verbose}{\code{TetradSearch$set_verbose()}}
\item \href{#method-TetradSearch-set_time_lag}{\code{TetradSearch$set_time_lag()}}
\item \href{#method-TetradSearch-get_data}{\code{TetradSearch$get_data()}}
\item \href{#method-TetradSearch-get_knowledge}{\code{TetradSearch$get_knowledge()}}
\item \href{#method-TetradSearch-get_java}{\code{TetradSearch$get_java()}}
\item \href{#method-TetradSearch-get_string}{\code{TetradSearch$get_string()}}
\item \href{#method-TetradSearch-get_dot}{\code{TetradSearch$get_dot()}}
\item \href{#method-TetradSearch-get_amat}{\code{TetradSearch$get_amat()}}
\item \href{#method-TetradSearch-clone}{\code{TetradSearch$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-new"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-new}{}}}
\subsection{Method \code{new()}}{
Initializes the \code{TetradSearch} object, creating new Java objects for
\code{knowledge} and \code{params}.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$new()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-set_test"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-set_test}{}}}
\subsection{Method \code{set_test()}}{
Sets the independence test to use in Tetrad.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$set_test(method, ..., mc = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{method}}{(character) Name of the test method (e.g., "chi_square", "fisher_z").
\itemize{
\item \code{"chi_square"} – chi-squared test
\item \code{"g_square"}   – likelihood-ratio \(G^2\) test
\item \code{"basis_function_lrt"} – basis-function likelihood-ratio
\item \code{"probabilistic"} – Uses BCInference by Cooper and Bui to calculate probabilistic conditional independence judgments.
\item \code{"fisher_z"} – Fisher \(Z\) (partial correlation) test
\item \code{"degenerate_gaussian"} – Degenerate Gaussian test as a likelihood ratio test
\item \code{"cci"} – Conditional independence of variable in a continuous data set using Daudin's method.
\item \code{"conditional_gaussian"} – Mixed discrete/continuous test
\item \code{"kci"} – Kernel Conditional Independence Test (KCI) by Kun Zhang
}}

\item{\code{...}}{Additional arguments passed to the private test-setting methods.
For the following tests, the following parameters are available:
\itemize{
\item \code{"chi_square"} – chi-squared test
\itemize{
\item \code{min_count = 1} – Minimum count for the chi-squared
test. Increasing this can improve accuracy of chi square
estimates,
\item \code{alpha = 0.01} – Significance level for the
chi-squared test,
\item \code{cell_table_type = 1} – The type of cell table to use
(optimization), 1 = AD Tree, 2 = Count Sample.
}
\item \code{"g_square"}   – likelihood-ratio \(G^2\) test
\itemize{
\item \code{min_count = 1} – Minimum count for the chi-squared
test. Increasing this can improve accuracy of chi square
estimates,
\item \code{alpha = 0.01} – Significance level for the
chi-squared test,
\item \code{cell_table_type = 1} – The type of cell table to use
(optimization), 1 = AD Tree, 2 = Count Sample.
}
\item \code{"basis_function_lrt"} – basis-function likelihood-ratio
\itemize{
\item \code{truncation_limit = 3} – Basis functions 1 through
this number will be used. The Degenerate Gaussian category
indicator variables for mixed data are also used,
\item \code{alpha = 0.01} – Significance level for the
likelihood-ratio test,
\item \code{singularity_lambda = 0.0} – Small number >= 0: Add
lambda to the the diagonal, < 0 Pseudoinverse.
}
\item \code{"probabilistic"} – Uses BCInference by Cooper and Bui to calculate probabilistic conditional independence judgments.
\itemize{
\item \code{threshold = FALSE} – Set to TRUE if using the cutoff
threshold for the independence test,
\item \code{cutoff = 0.5} – Cutoff for the independence test,
\item \code{prior_ess = 10} – Prior equivalent sample size
for the independence test. This number is added to the sample
size for each conditional probability table in the model and is
divided equally among the cells in the table.
}
\item \code{"fisher_z"} – Fisher \(Z\) (partial correlation) test
\itemize{
\item \code{alpha = 0.01} – Significance level
for the independence test,
\item \code{singularity_lambda = 0.0} – Small number >= 0: Add
lambda to the the diagonal, < 0 Pseudoinverse.
}
\item \code{"degenerate_gaussian"} – Degenerate Gaussian test as a likelihood ratio test
\itemize{
\item \code{alpha = 0.01} – Significance level for the
independence test,
\item \code{singularity_lambda = 0.0} – Small number >= 0: Add
lambda to the the diagonal, < 0 Pseudoinverse.
}
\item \code{"cci"} – Conditional independence of variable in a continuous data set using Daudin's method.
\itemize{
\item \code{alpha = 0.01} – Significance level for the
independence test,
\item \code{scaling_factor = 2} – For Gaussian kernel: The
scaling factor * Silverman bandwidth.
\item \code{basis_type = 4} – The type of basis function to use.
\itemize{
\item \code{1 = Polynomial},
\item \code{2 = Hermite1},
\item \code{3 = Hermite2},
\item \code{4 = Legendre},
\item \code{5 = Chebyshev}
}
\item \code{basis_scale = 0.0} – The scale of the basis function.
Variables are scaled to \verb{[-b, b]} for this b (0 = standardized).
\item \code{truncation_limit = 3} – Basis functions 1 through
this number will be used. The Degenerate Gaussian category
indicator variables for mixed data are also used.
}
\item \code{"conditional_gaussian"} – Mixed discrete/continuous test
\itemize{
\item \code{alpha = 0.01} – Significance level for the
independence test,
\item \code{discretize = TRUE} – If TRUE for the conditional
Gaussian likelihood, when scoring X --> D where X is continuous
and D discrete, one should to simply discretize X for just
those cases.
If FALSE, the integration will be exact,
\item \code{num_categories_to_discretize = 3} – In case the exact
algorithm is not used for discrete children and continuous
parents is not used, this parameter gives the number of
categories to use for this second (discretized) backup copy of
the continuous variables.
}
\item \code{"kci"} – Kernel Conditional Independence Test (KCI) by Kun Zhang
\itemize{
\item \code{alpha = 0.01} – Significance level for the
independence test,
\item \code{approximate = TRUE} – If TRUE, use the approximate
Gamma approximation algorithm. If FALSE, use the exact,
\item \code{scaling_factor = 1} – For Gaussian kernel: The
scaling factor * Silverman bandwidth.
\item \code{num_bootstraps = 5000} – Number of bootstrap
samples to use for the KCI test.
\item \code{threshold = 1e-3} – Threshold for the KCI test.
Threshold to determine how many eigenvalues to use --
the lower the more (0 to 1).
\item \code{kernel_type = "gaussian"} – The type of kernel to
use.
\itemize{
\item \code{1 = Gaussian},
\item \code{2 = Linear},
\item \code{3 = Polynomial},
}
\item \code{polyd = 5} – The degree of the polynomial kernel,
if used.
\item \code{polyc = 1} – The constant of the polynomial kernel,
if used.
}
}}

\item{\code{mc}}{(logical) If TRUE, sets this test for the Markov checker \code{mc_test}.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Invisibly returns \code{self}, for chaining.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-set_score"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-set_score}{}}}
\subsection{Method \code{set_score()}}{
Sets the scoring function to use in Tetrad.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$set_score(method, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{method}}{(character) Name of the score (e.g., "sem_bic", "ebic", "bdeu").
\itemize{
\item \code{"sem_bic"} – SEM BIC score.
\item \code{"ebic"} – Extended BIC score.
\item \code{"bdeu"} – Bayes Dirichlet Equivalent score with uniform priors.
\item \code{"basis_function_bic"} – BIC score for basis-function models. This is a generalization of the Degenerate Gaussian score.
\item \code{"conditional_gaussian"} – Mixed discrete/continuous BIC score.
\item \code{"degenerate_gaussian"} – Degenerate Gaussian BIC score.
\item \code{"discrete_bic"} – BIC score for discrete data.
\item \code{"gic"} – Generalized Information Criterion (GIC) score.
\item \code{"mag_degenerate_gaussian_bic"} – MAG Degenerate Gaussian BIC Score.
\item \code{"mixed_variable_polynomial"} – Mixed variable polynomial BIC score.
\item \code{"poisson_prior"} – Poisson prior score.
\item \code{"zhang_shen_bound"} – Gaussian Extended BIC score.
}}

\item{\code{...}}{Additional arguments passed to the private score-setting methods.
For the following scores, the following parameters are available:
\itemize{
\item \code{sem_bic} – SEM BIC score.
\itemize{
\item \code{penalty_discount = 2} – Penalty discount factor used in
BIC = 2L - ck log N, where c is the penalty. Higher c yield sparser
graphs,
\item \code{structure_prior = 0} – The default number of parents
for any conditional probability table. Higher weight is accorded
to tables with about that number of parents. The prior structure
weights are distributed according to a binomial distribution,
\item \code{sem_bic_rule = 1} – The Chickering Rule uses the
difference of BIC scores to add or remove edges. The Nandy et al.
rule uses a single calculation of a partial correlation in place
of the likelihood difference,
\item \code{precompute_covariances = TRUE} – For more than 5000
variables or so, set this to FALSE in order to calculate
covariances on the fly from data,
\item \code{singularity_lambda = 0.0} – Small number >= 0: Add
lambda to the the diagonal, < 0 Pseudoinverse
}
\item \code{ebic} – Extended BIC score.
\itemize{
\item \code{gamma} – The gamma parameter in the EBIC score.
\item \code{precompute_covariances = TRUE} – For more than 5000
variables or so, set this to FALSE in order to calculate
covariances on the fly from data,
\item \code{singularity_lambda = 0.0} – Small number >= 0: Add
lambda to the the diagonal, < 0 Pseudoinverse.
}
\item \code{bdeu} – Bayes Dirichlet Equivalent score with uniform priors.
\itemize{
\item \code{sample_prior = 10} – This sets the prior equivalent
sample size. This number is added to the sample size for each
conditional probability table in the model and is divided equally
among the cells in the table,
\item \code{singularity_lambda = 0.0} – Small number >= 0: Add
lambda to the the diagonal, < 0 Pseudoinverse.
}
\item \code{basis_function_bic} – BIC score for basis-function models. This is a generalization of the Degenerate Gaussian score.
\itemize{
\item \code{truncation_limit = 3} – Basis functions 1 though this
number will be used. The Degenerate Gaussian category indicator
variables for mixed data are also used,
\item \code{penalty_discount = 2} – Penalty discount. Higher penalty
yields sparser graphs,
\item \code{singularity_lambda = 0.0} – Small number >= 0: Add
lambda to the the diagonal, < 0 Pseudoinverse,
\item \code{do_one_equation_only = FALSE} – If TRUE, only one
equation should be used when expanding the basis.
}
\item \code{conditional_gaussian} – Mixed discrete/continuous BIC score.
\itemize{
\item \code{penalty_discount = 1} – Penalty discount. Higher penalty
yields sparser graphs,
\item \code{discretize = TRUE} – If TRUE for the conditional
Gaussian likelihood, when scoring X --> D where X is continuous and
D discrete, one should to simply discretize X for just those cases.
If FALSE, the integration will be exact,
\item \code{num_categories_to_discretize = 3} –  In case the exact
algorithm is not used for discrete children and continuous parents
is not used, this parameter gives the number of categories to use
for this second (discretized) backup copy of the continuous
variables,
\item \code{structure_prior = 0} – The default number of parents
for any conditional probability table. Higher weight is accorded
to tables with about that number of parents. The prior structure
weights are distributed according to a binomial distribution.
}
\item \code{"degenerate_gaussian"} – Degenerate Gaussian BIC score.
\itemize{
\item \code{penalty_discount = 1} – Penalty discount. Higher penalty
yields sparser graphs,
\item \code{structure_prior = 0} – The default number of parents
for any conditional probability table. Higher weight is accorded
to tables with about that number of parents. The prior structure
weights are distributed according to a binomial distribution,
\item \code{singularity_lambda = 0.0} – Small number >= 0: Add
lambda to the the diagonal, < 0 Pseudoinverse.
\item \code{precompute_covariances = TRUE} – For more than 5000
variables or so, set this to FALSE in order to calculate
covariances on the fly from data.
}
\item \code{"discrete_bic"} – BIC score for discrete data.
\itemize{
'item \code{penalty_discount = 2} – Penalty discount. Higher penalty
yields sparser graphs,
\item \code{structure_prior = 0} – The default number of parents
for any conditional probability table. Higher weight is accorded
to tables with about that number of parents. The prior structure
weights are distributed according to a binomial distribution.
}
\item \code{"gic"} – Generalized Information Criterion (GIC) score.
\itemize{
\item \code{penalty_discount = 1} – Penalty discount. Higher penalty
yields sparser graphs,
\item \code{sem_gic_rule = "bic"} – The following rules are available:
\itemize{
\item \code{"bic"} –  ln n,
\item \code{"gic2"} – pn^1/3,
\item \code{"ric"} –  2 ln pn,
\item \code{"ricc"} – 2(ln pn + ln ln pn),
\item \code{"gic5"} – ln ln n ln pn,
\item \code{"gic6"} – ln n ln pn
},
\item \code{precompute_covariances = TRUE} – For more than 5000
variables or so, set this to FALSE in order to calculate
covariances on the fly from data,
\item \code{singularity_lambda = 0.0} – Small number >= 0: Add
lambda to the the diagonal, < 0 Pseudoinverse.
}
\item \code{"mag_degenerate_gaussian_bic"} – MAG Degenerate Gaussian BIC Score.
\itemize{
\item \code{penalty_discount = 1} – Penalty discount. Higher penalty
yields sparser graphs,
\item \code{structure_prior = 0} – The default number of parents
for any conditional probability table. Higher weight is accorded
to tables with about that number of parents. The prior structure
weights are distributed according to a binomial distribution,
\item \code{precompute_covariances = TRUE} – For more than 5000
variables or so, set this to FALSE in order to calculate
covariances on the fly from data.
}
\item \code{"mixed_variable_polynomial"} – Mixed variable polynomial BIC score.
\itemize{
\item \code{structure_prior = 0} – The default number of parents
for any conditional probability table. Higher weight is accorded
to tables with about that number of parents. The prior structure
weights are distributed according to a binomial distribution,
\item \code{f_degree = 0} – The f degree.
\item \code{discretize = FALSE} – If TRUE for the conditional
Gaussian likelihood, when scoring X --> D where X is continuous and
D discrete, one should to simply discretize X for just those cases.
If FALSE, the integration will be exact.
}
\item \code{"poisson_prior"} – Poisson prior score.
\itemize{
\item \code{poission_lambda = 2} – Lambda parameter for the Poisson
distribution (> 0),
\item \code{precompute_covariances = TRUE} – For more than 5000
variables or so, set this to FALSE in order to calculate
covariances on the fly from data,
\item \code{singularity_lambda = 0.0} – Small number >= 0: Add
lambda to the the diagonal, < 0 Pseudoinverse.
}
\item \code{"zhang_shen_bound"} – Gaussian Extended BIC score.
\itemize{
\item \code{risk_bound = 0.2} – This is the probability of getting
the true model if a correct model is discovered. Could underfit.
\item \code{precompute_covariances = TRUE} – For more than 5000
variables or so, set this to FALSE in order to calculate
covariances on the fly from data,
\item \code{singularity_lambda = 0.0} – Small number >= 0: Add
lambda to the the diagonal, < 0 Pseudoinverse.
}
}}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Invisibly returns \code{self}.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-set_alg"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-set_alg}{}}}
\subsection{Method \code{set_alg()}}{
Sets the causal discovery algorithm to use in Tetrad.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$set_alg(method, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{method}}{(character) Name of the algorithm (e.g., "fges", "pc",
"fci", etc.).}

\item{\code{...}}{Additional parameters passed to the private algorithm-setting
methods.
For the following algorithms, the following parameters are available:
\itemize{
\item \code{"boss"} – BOSS algorithm.
\itemize{
\item \code{num_start = 1} – The number of times the algorithm
should be started from different initializations. By default, the
algorithm will be run through at least once using the initialized
parameters,
\item \code{use_bes = TRUE} – If TRUE, the algorithm uses the
backward equivalence search from the GES algorithm as one of its
steps,
\item \code{use_data_order = TRUE} – If TRUE, the data variable
order should be used for the first initial permutation,
\item \code{output_cpdag = TRUE} – BOSS can output a DAG or the
CPDAG of the DAG if FALSE.
}
\item \code{"boss_fci"} – BOSS-FCI algorithm.
\itemize{
\item \code{depth = -1} – Maximum size of conditioning set,
Set to -1 for unlimited,
\item \code{max_disc_path_length = -1} – Maximum length for any
discriminating path,
Set to -1 for unlimited,
\item \code{use_bes = TRUE} – If TRUE, the algorithm uses the
backward equivalence search from the GES algorithm as one of its
steps,
\item \code{use_heuristic} – If TRUE, use the max p heuristic
version,
\item \code{complete_rule_set_used = TRUE} –  FALSE if the (simpler)
final orientation rules set due to P. Spirtes, guaranteeing arrow
completeness, should be used; TRUE if the (fuller) set due to
J. Zhang, should be used guaranteeing additional tail completeness,
\item \code{guarantee_pag = FALSE} – Ensure the output is a legal PAG
(where feasible).
}
\item \code{"boss_pod"} – BOSS-POD (BOSS-PAG of DAG) algorithm.
\itemize{
\item \code{use_bes = TRUE} – If TRUE, the algorithm uses the
backward equivalence search from the GES algorithm as one of its
steps,
\item \code{use_data_order = TRUE} – If TRUE, the data variable
order should be used for the first initial permutation,
\item \code{num_starts = 1} – The number of times the algorithm
should be started from different initializations. By default, the
algorithm will be run through at least once using the initialized
parameters,
\item \code{complete_rule_set_used = TRUE} – FALSE if the (simpler)
final orientation rules set due to P. Spirtes, guaranteeing arrow
completeness, should be used; TRUE if the (fuller) set due to
J. Zhang, should be used guaranteeing additional tail completeness.
}
\item \code{"ccd"} – Cyclic Causal Discovery.
\itemize{
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{apply_r1 = TRUE} – Set this parameter to FALSE if a
chain of directed edges pointing in the same direction, when only
the first few such orientations are justified based on the data.
}
\item \code{"cfci"} – Adjusts FCI to use conservative orientation as in
CPC.
\itemize{
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{max_disc_path_length = -1} – Maximum length for any
discriminating path,
\item \code{complete_rule_set_used = TRUE} – FALSE if the (simpler)
final orientation rules set due to P. Spirtes, guaranteeing arrow
completeness, should be used; TRUE if the (fuller) set due to
J. Zhang, should be used guaranteeing additional tail completeness.
}
\item \code{"cpc"} – Conservative PC algorithm.
\itemize{
\item \code{conflict_rule = 1} –
\itemize{
\item \code{1} – The “overwrite” rule as introduced in the
\code{\link[pcalg:pc]{pcalg}} R package,
\item \code{2} – All collider conflicts using bidirected edges
should be prioritized,
\item \code{3} – Existing colliders should be prioritized,
ignoring subsequent conflicting information,
}
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{stable_fas = TRUE} – If TRUE, the "stable" version of
the PC adjacency search is used, which for k > 0 fixes the graph
for depth k + 1 to that of the previous depth k.
\item \code{guarantee_cpdag = FALSE} – If TRUE, ensure the output is
a legal CPDAG.
}
\item \code{"cstar"} – CStaR algorithm (Causal Stability Ranking).
\itemize{
\item \code{targets = ""} – Target names (comma or space separated),
\item \code{file_out_path = "cstar_out"} –  Path to a directory in
which results can be stored
\item \code{selection_min_effect = 0.0} – Minimum effect size for
listing effects in the CStaR table
\item \code{num_subsamples = 50} –  CStaR works by generating
subsamples and summarizing across them;
this specifies the number of subsamples to generate.
Must be >= 1,
\item \code{top_bracket = 10} – Top bracket to look for causes in,
\item \code{parallelized = FALSE} – If TRUE, the algorithm should
be parallelized,
\item \code{cpdag_algorithm = 4} – The CPDAG algorithm to use:
\itemize{
\item \code{1} – PC Stable,
\item \code{2} – FGES,
\item \code{3} – BOSS,
\item \code{4} – Restricted BOSS.
}
\item \code{remove_effect_nodes = TRUE} – If TRUE, the effect nodes
should be removed from possible causes,
\item \code{sample_style = 1} – The sampling style to use:
\itemize{
\item \code{1} – Subsample
\item \code{2} – Bootstrap
}
}
\item \code{"dagma"} – DAGMA algorithm.
\itemize{
\item \code{lambda1 = 0.05} – Tuning parameter for DAGMA,
\item \code{w_threshold - 0.1} – Second tuning parameter for DAGMA,
\item \code{cpdag = TRUE} – The algorithm returns a DAG;
if this is set to TRUE, this DAG is converted to a CPDAG.
}
\item \code{"direct_lingam"} – DirectLiNGAM algorithm. No parameters.
\item \code{"fask"} – FASK algorithm.
\itemize{
\item \code{alpha = 0.05} – Significance level for the
independence test,
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{fask_delta = -0.3} – The bias for orienting with
negative coefficients (\code{0} means no bias) for \code{FASK v1},
\item \code{left_right_rule = 1} – The FASK left right rule v2 is
default, but two other (related) left-right rules are given for
relation to the literature, and the v1 FASK rule is included for
backward compatibility,
\item \code{skew_edge_threshold 0.3} – For FASK, this includes an
adjacency X --- Y in the model if
|corr(X, Y | X > 0) – corr(X, Y | Y > 0)|
exceeds some threshold.
}
\item \code{"fci"} – FCI algorithm.
\itemize{
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{stable_fas = TRUE} – If TRUE, the "stable" version of
the PC adjacency search is used, which for k > 0 fixes the graph
for depth k + 1 to that of the previous depth k.
\item \code{max_disc_path_length = -1} – Maximum length for any
discriminating path,
\item \code{complete_rule_set_used = TRUE} – FALSE if the (simpler)
final orientation rules set due to P. Spirtes, guaranteeing arrow
completeness, should be used; TRUE if the (fuller) set due to
J. Zhang, should be used guaranteeing additional tail completeness.
\item \code{guarantee_pag = FALSE} – Ensure the output is a legal
PAG (where feasible).
}
\item \code{"fcit"} – FCI Targeted Testing (FCIT) algorithm
\itemize{
\item \code{use_bes = TRUE} – If TRUE, the algorithm uses the
backward equivalence search from the GES algorithm as one of its
steps,
\item \code{use_data_order = TRUE} – If TRUE, the data variable
order should be used for the first initial permutation,
\item \code{num_starts = 1} – The number of times the algorithm
should be started from different initializations. By default, the
algorithm will be run through at least once using the initialized
parameters,
\item \code{start_with = "BOSS"} – What algorithm to run first to get
the initial CPDAG that the rest of the FCIT procedure refines.
\itemize{
\item \code{BOSS}
\item \code{GRaSP}
\item \code{SP}
}
\item \code{check_adjacency_sepsets = FALSE} – If TRUE, the
condition sets should at the end be checked that are subsets of
adjacencies of the variables. This is only done after all recursive
sepset removals have been done. True by default. This is needed in
order to pass an Oracle test, but can reduce accuracy from data.
\item \code{complete_rule_set_used = TRUE} – FALSE if the (simpler)
final orientation rules set due to P. Spirtes, guaranteeing arrow
completeness, should be used; TRUE if the (fuller) set due to
J. Zhang, should be used guaranteeing additional tail completeness,
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{guarantee_pag = FALSE} – Ensure the output is a legal
PAG (where feasible).
}
\item \code{"fges"} – Fast Greedy Equivalence Search (FGES) algorithm.
\itemize{
\item \code{symmetric_first_step = FALSE} – If TRUE, scores for both
X --> Y and X <-- Y will be calculated and the higher score used.
\item \code{max_degree = -1} – Maximum degree of any node in the
graph. Set to -1 for unlimited,
\item \code{parallelized = FALSE} – If TRUE, the algorithm should
be parallelized,
\item \code{faithfulness_assumed = FALSE} – If TRUE, assume that if
X \emph{||} Y (by an independence test) then X \emph{||} Y | Z for nonempty
Z.
}
\item \code{"fges_mb"} – Fast Greedy Equivalence Search with Markov
Blanket (FGES-MB) algorithm.
\itemize{
\item \code{targets = ""} – Target names (comma or space separated),
\item \code{max_degree = -1} – Maximum degree of any node in the
graph. Set to -1 for unlimited,
\item \code{trimming_style = 3} –  'Adjacencies' trims to the
adjacencies the targets, MB DAGs to the
Union(MB(targets)) U targets, semidirected trims to nodes with
semidirected paths to the targets. The trimming style to use:
\itemize{
\item \code{1} – No trimming,
\item \code{2} – Adjacencies,
\item \code{3} – MB DAGs,
\item \code{4} – Semidirected paths,
}
\item \code{number_of_expansions = 2} – Number of expansions of the
algorithm away from the targetv
\item \code{faithfulness_assumed = FALSE} – If TRUE, assume that if
X \emph{||} Y (by an independence test) then X \emph{||} Y | Z for nonempty
Z.
}
\item \code{"fofc"} – Find One Factor Clusters (FOFC)/MIMBUILD
algorithm.
\itemize{
\item \code{alpha = 0.001} – Cutoff for p values (alpha),
\item \code{penalty_discount = 2.0} – Penalty discount factor used
in BIC = 2L - ck log N, where c is the penalty. Higher c yields
sparser graphs,
\item \code{tetrad_test = 1} –  The tetrad test used:
\itemize{
\item \code{1} – CCA,
\item \code{2} – Bollen-Ting,
\item \code{3} – Wishart,
\item \code{4} – Ark,
}
\item \code{include_structure_model = TRUE} – If \code{TRUE}
FOFC goes beyond the clustering step and calls a MIMBUILD routine
(PCA or Bollen, chosen via \code{mimbuild_type}) to learn the
causal relationships \strong{between} the latent factors.
The returned graph then contains both the measurement model
(latent --> indicator edges) \emph{and} the latent-level structure.
When \code{FALSE} FOFC stops after the clustering phase and returns
only the measurement model.
\item \code{precompute_covariances = TRUE} – For more than 5000
variables or so, set this to FALSE in order to calculate
covariances on the fly from data.
}
\item \code{"gfci"} – GFCI algorithm. Combines FGES and FCI.
\itemize{
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{max_degree = -1} – Maximum degree of any node in the
graph. Set to -1 for unlimited,
\item \code{max_disc_path_length = -1} – Maximum length for any
discriminating path,
\item \code{complete_rule_set_used = TRUE} – FALSE if the (simpler)
final orientation rules set due to P. Spirtes, guaranteeing arrow
completeness, should be used; TRUE if the (fuller) set due to
J. Zhang, should be used guaranteeing additional tail completeness.
\item \code{guarantee_pag = FALSE} – Ensure the output is a legal
PAG (where feasible).
}
\item \code{"grasp"} – GRaSP (Greedy Relations of Sparsest Permutation)
algorithm.
\itemize{
\item \code{covered_depth = 4} – The depth of recursion for first
search,
\item \code{singular_depth = 1} – Recursion depth for singular
tucks,
\item \code{nonsingular_depth = 1} – Recursion depth for nonsingular
tucks,
\item \code{ordered_alg = FALSE} – If TRUE, earlier GRaSP stages
should be performed before later stages,
\item \code{raskutti_uhler = FALSE} – If TRUE, use Raskutti and
Uhler's DAG-building method (test); if FALSE, use Grow-Shrink
(score).
\item \code{use_data_order = TRUE} – If TRUE, the data variable
order should be used for the first initial permutation,
\item \code{num_starts = 1} – The number of times the algorithm
should be started from different initializations. By default, the
algorithm will be run through at least once using the initialized
parameters.
}
\item \code{"grasp_fci"} – GRaSP-FCI algorithm. Combines GRaSP and FCI.
\itemize{
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{stable_fas = TRUE} – If TRUE, the "stable" version of
the PC adjacency search is used, which for k > 0 fixes the graph
for depth k + 1 to that of the previous depth k.
\item \code{max_disc_path_length = -1} – Maximum length for any
discriminating path,
\item \code{complete_rule_set_used = TRUE} – FALSE if the (simpler)
final orientation rules set due to P. Spirtes, guaranteeing arrow
completeness, should be used; TRUE if the (fuller) set due to
J. Zhang, should be used guaranteeing additional tail completeness,
\item \code{covered_depth = 4} – The depth of recursion for first
search,
\item \code{singular_depth = 1} – Recursion depth for singular
tucks,
\item \code{nonsingular_depth = 1} – Recursion depth for nonsingular
tucks,
\item \code{ordered_alg = FALSE} – If TRUE, earlier GRaSP stages
should be performed before later stages,
\item \code{raskutti_uhler = FALSE} – If TRUE, use Raskutti and
Uhler's DAG-building method (test); if FALSE, use Grow-Shrink
(score).
\item \code{use_data_order = TRUE} – If TRUE, the data variable
order should be used for the first initial permutation,
\item \code{num_starts = 1} – The number of times the algorithm
should be started from different initializations. By default, the
algorithm will be run through at least once using the initialized
parameters,
\item \code{guarantee_pag = FALSE} – If TRUE, ensure the output is a
legal PAG (where feasible).
}
\item \code{"ica_lingam"} – ICA LiNGAM algorithm.
\itemize{
\item \code{ica_a = 1.1} – The 'a' parameter of Fast ICA
(see Hyvarinen, A. (2001)). It ranges between 1 and 2.
\item \code{ica_max_iter = 5000} –  Maximum number if iterations of
the optimization procedure of ICA.
\item \code{ica_tolerance = 1e-8} – Fast ICA tolerance parameter.
\item \code{threshold_b = 0.1} – The estimated B matrix is
thresholded by setting small entries less than this threshold to
zero.
}
\item \code{"ica_lingd"} – ICA-LiNG-D algorithm
\itemize{
\item \code{ica_a = 1.1} – The 'a' parameter of Fast ICA
(see Hyvarinen, A. (2001)). It ranges between 1 and 2.
\item \code{ica_max_iter = 5000} –  Maximum number if iterations of
the optimization procedure of ICA.
\item \code{ica_tolerance = 1e-8} – Fast ICA tolerance parameter.
\item \code{threshold_b = 0.1} – The estimated B matrix is
thresholded by setting small entries less than this threshold to
zero.
\item \code{threshold_w} – The estimated W matrix is thresholded by
setting small entries less than this threshold to zero.
}

\item \code{"pc"} – Peter-Clark (PC) algorithm
\itemize{
\item \code{conflict_rule = 1} –
\itemize{
\item \code{1} – The “overwrite” rule as introduced in the
\code{\link[pcalg:pc]{pcalg}} R package,
\item \code{2} – All collider conflicts using bidirected edges
should be prioritized,
\item \code{3} – Existing colliders should be prioritized,
ignoring subsequent conflicting information,
}
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{stable_fas = TRUE} – If TRUE, the "stable" version of
the PC adjacency search is used, which for k > 0 fixes the graph
for depth k + 1 to that of the previous depth k.
\item \code{guarantee_cpdag = FALSE} – If TRUE, ensure the output is
a legal CPDAG.
}
\item \code{"pc_max"} – PCMax algorithm
\itemize{
\item \code{conflict_rule = 1} –
\itemize{
\item \code{1} – The “overwrite” rule as introduced in the
\code{\link[pcalg:pc]{pcalg}} R package,
\item \code{2} – All collider conflicts using bidirected edges
should be prioritized,
\item \code{3} – Existing colliders should be prioritized,
ignoring subsequent conflicting information,
}
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{use_heuristic = TRUE} – If TRUE, use the max p heuristic
version
\item \code{max_disc_path_length = -1} – The maximum path length to
use for the max p heuristic version. If -1, no limit is used.
\item \code{stable_fas = TRUE} – If TRUE, the "stable" version of
the PC adjacency search is used, which for k > 0 fixes the graph
for depth k + 1 to that of the previous depth k.
}
\item \code{"restricted_boss"} – Restricted BOSS algorithm
\itemize{
\item \code{targets = ""} – Target names (comma or space separated),
\item \code{use_bes = TRUE} – If TRUE, the algorithm uses the
backward equivalence search from the GES algorithm as one of its
steps,
\item \code{num_starts = 1} – The number of times the algorithm
should be started from different initializations. By default, the
algorithm will be run through at least once using the initialized
parameters,
\item \code{allow_internal_randomness = TRUE} –  If TRUE, the
algorithm allow the algorithm to use certain heuristic random
steps. This can improve performance, but may make the algorithm
non-deterministic.
}
\item \code{"rfci"} – Restricted FCI algorithm
\itemize{
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{stable_fas = TRUE} – If TRUE, the "stable" version of
the PC adjacency search is used, which for k > 0 fixes the graph
for depth k + 1 to that of the previous depth k.
\item \code{max_disc_path_length = -1} – Maximum length for any
discriminating path,
\item \code{complete_rule_set_used = TRUE} – FALSE if the (simpler)
final orientation rules set due to P. Spirtes, guaranteeing arrow
completeness, should be used; TRUE if the (fuller) set due to
J. Zhang, should be used guaranteeing additional tail completeness.
\item \code{guarantee_pag = FALSE} – Ensure the output is a legal
PAG (where feasible).
}
\item \code{"sp"} – Sparsest Permutation algorithm. No parameters.
\item \code{"sp_fci"} – Sparsest Permutation using FCI
\itemize{
\item \code{depth = -1} – Maximum size of conditioning set,
\item \code{max_disc_path_length = -1} – Maximum length for any
discriminating path,
\item \code{complete_rule_set_used = TRUE} – FALSE if the (simpler)
final orientation rules set due to P. Spirtes, guaranteeing arrow
completeness, should be used; TRUE if the (fuller) set due to
J. Zhang, should be used guaranteeing additional tail completeness,
\item \code{guarantee_pag = FALSE} – Ensure the output is a legal
PAG (where feasible).
}
\item \code{"svar_fci"} – SvarFCI algorithm (for timeseries data)
\itemize{
\item \code{penalty_discount = 2} – Penalty discount factor used
in BIC = 2L - ck log N, where c is the penalty. Higher c yields
sparser graphs.
}
\item \code{"svar_gfci"} – SvarGFCI algorithm. Similar to SvarFCI,
but uses a BIC score to search for a skeleton.
\itemize{
\item \code{penalty_discount = 2} – Penalty discount factor used
in BIC = 2L - ck log N, where c is the penalty. Higher c yields
sparser graphs.
}
}}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Invisibly returns \code{self}.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-set_knowledge"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-set_knowledge}{}}}
\subsection{Method \code{set_knowledge()}}{
Sets the background knowledge object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$set_knowledge(knowledge_obj)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{knowledge_obj}}{An object containing Tetrad knowledge (must implement \code{get_tetrad_knowledge}).}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-set_params"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-set_params}{}}}
\subsection{Method \code{set_params()}}{
Sets parameters for the Tetrad search.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$set_params(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Named arguments for the parameters to set.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-get_parameters_for_function"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-get_parameters_for_function}{}}}
\subsection{Method \code{get_parameters_for_function()}}{
Retrieves the argument names of a matching private function.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$get_parameters_for_function(
  fn_pattern,
  score = FALSE,
  test = FALSE,
  alg = FALSE
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{fn_pattern}}{(character) A pattern that should match a private method name.}

\item{\code{score}}{If TRUE, retrieves parameters for a scoring function.}

\item{\code{test}}{If TRUE, retrieves parameters for a test function.}

\item{\code{alg}}{If TRUE, retrieves parameters for an algorithm.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
(character) The names of the parameters.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-run_search"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-run_search}{}}}
\subsection{Method \code{run_search()}}{
Runs the chosen Tetrad algorithm on the data.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$run_search(
  data = NULL,
  bootstrap = FALSE,
  bhat = FALSE,
  unstable_bhat = FALSE,
  stable_bhat = FALSE
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{data}}{(optional) If provided, overrides the previously set data.}

\item{\code{bootstrap}}{(logical) If TRUE, bootstrapped graphs will be generated.}

\item{\code{bhat}}{(logical) If TRUE, retrieve the BHat adjacency matrix.}

\item{\code{unstable_bhat}}{(logical) If TRUE, retrieve unstable BHats.}

\item{\code{stable_bhat}}{(logical) If TRUE, retrieve stable BHats.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
discography with graph. Also populates \code{self$java}.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-set_bootstrapping"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-set_bootstrapping}{}}}
\subsection{Method \code{set_bootstrapping()}}{
Configures bootstrapping parameters for the Tetrad search.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$set_bootstrapping(
  number_resampling = 0,
  percent_resample_size = 100,
  add_original = TRUE,
  with_replacement = TRUE,
  resampling_ensemble = 1,
  seed = -1
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{number_resampling}}{(integer) Number of bootstrap samples.}

\item{\code{percent_resample_size}}{(numeric) Percentage of sample size for each bootstrap.}

\item{\code{add_original}}{(logical) If TRUE, add the original dataset to the bootstrap set.}

\item{\code{with_replacement}}{(logical) If TRUE, sampling is done with replacement.}

\item{\code{resampling_ensemble}}{(integer) How the resamples are used or aggregated.}

\item{\code{seed}}{(integer) Random seed, or -1 for none.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-set_data"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-set_data}{}}}
\subsection{Method \code{set_data()}}{
Sets or overrides the data used by Tetrad.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$set_data(data)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{data}}{(data.frame) The new data to load.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-set_verbose"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-set_verbose}{}}}
\subsection{Method \code{set_verbose()}}{
Toggles the verbosity in Tetrad.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$set_verbose(verbose)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{verbose}}{(logical) TRUE to enable verbose logging, FALSE otherwise.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-set_time_lag"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-set_time_lag}{}}}
\subsection{Method \code{set_time_lag()}}{
Sets an integer time lag for time-series algorithms.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$set_time_lag(time_lag = 0)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{time_lag}}{(integer) The time lag to set.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-get_data"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-get_data}{}}}
\subsection{Method \code{get_data()}}{
Retrieves the current Java data object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$get_data()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
(Java object) Tetrad dataset.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-get_knowledge"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-get_knowledge}{}}}
\subsection{Method \code{get_knowledge()}}{
Returns the background knowledge object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$get_knowledge()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
(Java object) Tetrad Knowledge.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-get_java"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-get_java}{}}}
\subsection{Method \code{get_java()}}{
Gets the main Java result object (usually a graph) from the last search.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$get_java()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
(Java object) The Tetrad result graph or model.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-get_string"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-get_string}{}}}
\subsection{Method \code{get_string()}}{
Returns the string representation of a given Java object or \code{self$java}.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$get_string(java_obj = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{java_obj}}{(Java object, optional) If NULL, uses \code{self$java}.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
(character) The \code{toString()} of that Java object.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-get_dot"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-get_dot}{}}}
\subsection{Method \code{get_dot()}}{
Produces a DOT (Graphviz) representation of the graph.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$get_dot(java_obj = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{java_obj}}{(Java object, optional) If NULL, uses \code{self$java}.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
(character) The DOT-format string.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-get_amat"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-get_amat}{}}}
\subsection{Method \code{get_amat()}}{
Produces an amat representation of the graph.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$get_amat(java_obj = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{java_obj}}{(Java object, optional) If NULL, uses \code{self$java}.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
(character) The adjacency matrix.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TetradSearch-clone"></a>}}
\if{latex}{\out{\hypertarget{method-TetradSearch-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TetradSearch$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
